{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from brits2_i_original.ipynb\n",
      "importing Jupyter notebook from rits2_i_original.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random \n",
    "import torch as T\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, IterableDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "import import_ipynb\n",
    "from brits2_i_original import *\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "save_path = \"data/saved_models/activityReverse.tar\"#vaegan_model - Copy.tar\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "if not os.path.exists(\"data/saved_models\"):\n",
    "    os.makedirs(\"data/saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval'], dest='eval', nargs=None, const=None, default=True, type=None, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARG_PARSER = ArgumentParser()\n",
    "\n",
    "ARG_PARSER.add_argument('--nfeatures', default=609, type=int)\n",
    "ARG_PARSER.add_argument('--dfeatures', default=43, type=int)\n",
    "ARG_PARSER.add_argument('--ehidden', default=300, type=int)\n",
    "ARG_PARSER.add_argument('--model', type=str)\n",
    "\n",
    "ARG_PARSER.add_argument('--ehr', default=True)\n",
    "ARG_PARSER.add_argument('--air', default=False)\n",
    "ARG_PARSER.add_argument('--mimic', default=False)\n",
    "\n",
    "ARG_PARSER.add_argument('--num_epochs', default=100, type=int)\n",
    "ARG_PARSER.add_argument('--seq_len', default=40, type=int)\n",
    "ARG_PARSER.add_argument('--pred_len', default=8, type=int)\n",
    "ARG_PARSER.add_argument('--batch_size', default=200, type=int)\n",
    "ARG_PARSER.add_argument('--missingRate', default=10, type=int)\n",
    "ARG_PARSER.add_argument('--patience', default=30, type=int)\n",
    "ARG_PARSER.add_argument('--e_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--g_lrn_rate', default=0.1, type=float)\n",
    "ARG_PARSER.add_argument('--d_lrn_rate', default=0.001, type=float)\n",
    "ARG_PARSER.add_argument('--resume_training', default=False)\n",
    "ARG_PARSER.add_argument('--train', default=True)\n",
    "ARG_PARSER.add_argument('--evalImp', default=False)\n",
    "ARG_PARSER.add_argument('--evalPred', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS = ARG_PARSER.parse_args(args=[])\n",
    "MAX_SEQ_LEN = ARGS.seq_len\n",
    "BATCH_SIZE = ARGS.batch_size\n",
    "EPSILON = 1e-40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, path, chunksize,length,seq_len,flag):\n",
    "        self.path = path\n",
    "        self.chunksize = chunksize\n",
    "        self.len = int(length)#number of times total getitem is called\n",
    "        self.seq_len=seq_len\n",
    "        self.flag=flag\n",
    "        self.reader=pd.read_csv(\n",
    "                self.path,header=0,\n",
    "                chunksize=self.chunksize)#,names=['data']))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.reader.get_chunk(self.chunksize)\n",
    "        #sex=pd.read_csv('C:\\\\Users/mehak/Desktop/demo.csv',header=0)\n",
    "        #sex=sex[['person_id','Sex']]\n",
    "        #data = pd.merge(data, sex, how='left', on=['person_id'])\n",
    "        #print(data.shape)\n",
    "        #data=data.sort_values(by=['RANDOM_PATIENT_ID','VISIT_YEAR','VISIT_MONTH'])\n",
    "        #print(data['RANDOM_PATIENT_ID'].unique())\n",
    "#         del data['person_id']\n",
    "#         print(data.columns.get_loc('BMI'))\n",
    "        #print(data.columns)\n",
    "\n",
    "        data=data.replace(np.inf,0)\n",
    "        data=data.replace(np.nan,0)\n",
    "        data=data.fillna(0)\n",
    "        #print(data.shape)\n",
    "        if(self.flag==0):\n",
    "#             data['Age']=data['Age'].apply(lambda x: ((x*12)/3)-81)\n",
    "            pids=data['person_id']\n",
    "#             age=data['age']\n",
    "#             del data['age']\n",
    "            pids = T.as_tensor(pids.values.astype(float), dtype=T.long)\n",
    "#             age = T.as_tensor(age.values.astype(float), dtype=T.long)\n",
    "#             print(\"age\",data['Age'])\n",
    "#             print(\"pids\",list(pids))\n",
    "#             print(\"========================================================\")\n",
    "\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "    #         print(list(data[:,0]))\n",
    "    #         print(\"========================================================\")\n",
    "            #data=T.from_numpy(data)\n",
    "            #data=data.double()\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            #print(data.shape)\n",
    "            #print(\"age\",data[0,:,203])\n",
    "            #mask=pd.DataFrame()\n",
    "            #mask = data.loc[data['LABS_LDL_MEAN']>0,'LABS_LDL_MEAN']=1\n",
    "            #df[df['LABS_LDL_MEAN']<0].count()\n",
    "            return data,pids\n",
    "        else:\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            \n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf#11.1179\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, optimizer, save_path):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, save_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, save_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, optimizer, save_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        T.save({\n",
    "            \"model\": model.state_dict(),\n",
    "            'trainer': optimizer.state_dict()\n",
    "        }, save_path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_test(args, model,predWin):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    FLoss=0\n",
    "    mseLoss=0\n",
    "    mseLossF=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    oBmiF=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    imputations=[]\n",
    "    with T.autograd.no_grad():\n",
    "        for i in ['M','F']:\n",
    "            files = 'cond'+i+'test.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+G+'val.csv'\n",
    "\n",
    "            maskFiles = 'mask'+i+'test.csv'\n",
    "            #print(files)\n",
    "\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            #orig = CSVDataset('C:\\\\Users/mehak/Desktop/testganAggOrig.csv', int(args.seq_len*500),1356100,args.seq_len)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            #origLoader = DataLoader(orig,batch_size=1,num_workers=0, shuffle=False)\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "            loss={}\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]   \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "    #             print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "            \n",
    "                #values to be predicted\n",
    "                y = data.clone().detach()\n",
    "#                 print(y.shape)\n",
    "#                 print(data.shape)\n",
    "                testMask = mask.clone().detach()\n",
    "    #             sex=y[:,:,608]\n",
    "    #             age=y[:,:,607]\n",
    "                #print(sex.shape,age.shape)\n",
    "                y=y[:,:,811]\n",
    "\n",
    "                #------------remove last 5 timestamps------------------\n",
    "                #print(data[0:10,8:,653])\n",
    "                for i in range(data.shape[0]):\n",
    "                    #if(data[i,])\n",
    "                    j=40\n",
    "                    if(predWin==8):\n",
    "                        k=32\n",
    "                    elif(predWin==7):\n",
    "                        k=28\n",
    "                    elif(predWin==6):\n",
    "                        k=24\n",
    "                    elif(predWin==5):\n",
    "                        k=20\n",
    "\n",
    "                    data[i,j-k:j,:]=0\n",
    "                    mask[i,j-k:j]=0\n",
    "                    y[i,0:j-k]=0\n",
    "    #                 age[i,0:j-k]=0\n",
    "    #                 sex[i,0:j-k]=0\n",
    "                    #print(sex.shape,age.shape)\n",
    "                    #yOrig[i,0:j-k]=0\n",
    "                    testMask[i,0:j-k]=0\n",
    "\n",
    "\n",
    "                ret_f, ret = run_on_batch(model,data,mask,decay,rdecay, args, optimizer=None,epoch=None)#,bmi_norm)\n",
    "                #print(\"Input\",data.shape)\n",
    "                #print(data[0,:,316])\n",
    "                #print(\"Reverse\",ret['imputations'][0,:])\n",
    "                #print(\"ForwardOnly\",ret_f['imputations'][0,:])\n",
    "                #print(\"Original\",y.shape)\n",
    "                #print(y[0,:])\n",
    "                #print(\"Mask\",testMask.shape)\n",
    "                #print(testMask[0,:])\n",
    "                RLoss=RLoss+ret['loss']\n",
    "                FLoss=FLoss+ret_f['loss']\n",
    "#                 testMask=testMask.cuda()\n",
    "#                 y=y.cuda()\n",
    "                outputBMI=ret['imputations'] * testMask\n",
    "                outputBMIF=ret_f['imputations'] * testMask\n",
    "                mseLoss=mseLoss+ (torch.sum(torch.abs(outputBMI-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                mseLossF=mseLossF+ (torch.sum(torch.abs(outputBMIF-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                #print(\"RMSELoss Revrese: \",mseLoss)\n",
    "                #print(\"RMSELoss Forward: \",mseLossF)\n",
    "                outBmi, outBmiF,inBmi = plotBmi(outputBMI, outputBMIF, y, testMask)\n",
    "                oBmi.extend(outBmi)\n",
    "                oBmiF.extend(outBmiF)\n",
    "                iBmi.extend(inBmi)\n",
    "\n",
    "                #T.cuda.empty_cache()\n",
    "                #paramsE=list(model['e'].parameters())\n",
    "                #paramsG=list(model['g'].parameters())\n",
    "                #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])  \n",
    "            TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "        mseLoss = mseLoss/TBatches\n",
    "        mseLossF = mseLossF/TBatches\n",
    "    #print(\"===================================\")\n",
    "    oBmi=np.asarray(oBmi)\n",
    "    iBmi=np.asarray(iBmi)\n",
    "    loss = (oBmi - iBmi)\n",
    "    loss=np.asarray([abs(number) for number in loss])\n",
    "    variance = sum([((x - mseLoss) ** 2) for x in loss]) / len(loss) \n",
    "    res = variance ** 0.5\n",
    "    ci=1.96*(res/(math.sqrt(len(loss))))\n",
    "\n",
    "    #print(\"Val R Loss:\",RLoss)\n",
    "    print(\"CI\",ci)\n",
    "    print(\"MAE Loss Reverse:\",mseLoss)\n",
    "    print(\"MAE Loss Forward:\",mseLossF)\n",
    "    #print(outputBMI)\n",
    "    return oBmi,oBmiF,iBmi\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation_test(args, model,missingRate):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    FLoss=0\n",
    "    mseLoss=0\n",
    "    mseLossF=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    oBmiF=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "    imputations=[]\n",
    "    samples=0\n",
    "    pids=0\n",
    "    with T.autograd.no_grad():\n",
    "        for i in ['M','F']:\n",
    "            files = 'cond'+i+'test.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+G+'val.csv'\n",
    "\n",
    "            maskFiles = 'mask'+i+'test.csv'\n",
    "            #print(files)\n",
    "\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            #orig = CSVDataset('C:\\\\Users/mehak/Desktop/testganAggOrig.csv', int(args.seq_len*500),1356100,args.seq_len)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            #origLoader = DataLoader(orig,batch_size=1,num_workers=0, shuffle=False)\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "            loss={}\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]   \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "    #             print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "            \n",
    "                #values to be predicted\n",
    "                y = data.clone().detach()\n",
    "#                 print(y.shape)\n",
    "#                 print(data.shape)\n",
    "                testMask = mask.clone().detach()\n",
    "    #             sex=y[:,:,608]\n",
    "    #             age=y[:,:,607]\n",
    "                #print(sex.shape,age.shape)\n",
    "                y=y[:,:,811]\n",
    "            \n",
    "                bmi=811\n",
    "\n",
    "\n",
    "                #------------remove last 5 timestamps------------------\n",
    "                #print(data[0:10,8:,653])\n",
    "                for i in range(data.shape[0]):\n",
    "                    #if(data[i,])\n",
    "                    #mask[i,:].loc[mask[i,:].query('value == 1').sample(frac=.1).index,'value'] = 0\n",
    "                    idxs = torch.nonzero(mask[i,:] == 1)\n",
    "                    samples=samples+list(idxs.size())[0]\n",
    "                    if((missingRate==50) & (list(idxs.size())[0]>4)):\n",
    "                        idxs=random.sample(set(idxs),5)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        data[i,idxs[2],bmi]=0\n",
    "                        data[i,idxs[3],bmi]=0\n",
    "                        data[i,idxs[4],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        mask[i,idxs[3]]=0\n",
    "                        mask[i,idxs[4]]=0\n",
    "                        pids=pids + 5\n",
    "                    elif((missingRate>=40) & (list(idxs.size())[0]>3)):\n",
    "                        idxs=random.sample(set(idxs),4)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        data[i,idxs[2],bmi]=0\n",
    "                        data[i,idxs[3],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        mask[i,idxs[3]]=0\n",
    "                        pids=pids + 4\n",
    "                    elif((missingRate>=30) & (list(idxs.size())[0]>2)):\n",
    "                        idxs=random.sample(set(idxs),3)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        data[i,idxs[2],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        mask[i,idxs[2]]=0\n",
    "                        pids=pids + 3\n",
    "                    elif((missingRate>=20) & (list(idxs.size())[0]>1)):\n",
    "                        idxs=random.sample(set(idxs),2)\n",
    "                        data[i,idxs[0],bmi]=0\n",
    "                        data[i,idxs[1],bmi]=0\n",
    "                        #print(mask[i,:])\n",
    "                        mask[i,idxs[0]]=0\n",
    "                        mask[i,idxs[1]]=0\n",
    "                        pids=pids + 2\n",
    "                    elif((missingRate>=10) & (list(idxs.size())[0]>0)):\n",
    "                        if (i%2==0):\n",
    "                            idxs=random.sample(set(idxs),1)\n",
    "                            data[i,idxs,bmi]=0\n",
    "                            mask[i,idxs]=0\n",
    "                            pids=pids + 1\n",
    "\n",
    "                    #data[i,idxs,316]=0\n",
    "                    #print(mask[i,:])\n",
    "                    #mask[i,idxs]=0\n",
    "                    #print(mask[i,:])\n",
    "                    testMask[i,:]=testMask[i,:]-mask[i,:]\n",
    "                    #print(testMask[i,:])\n",
    "                    #print(y[i,:])\n",
    "                    y[i,:]=y[i,:]*testMask[i,:]\n",
    "\n",
    "\n",
    "                #print(\"Input Data\",data.shape)\n",
    "                #print(\"Input Mask\",mask.shape)\n",
    "                ret_f,ret = run_on_batch(model,data,mask,decay,rdecay, args, optimizer=None,epoch=None)#,bmi_norm)\n",
    "                #print(\"Input\",data.shape)\n",
    "                #print(data[0,:,316])\n",
    "                #print(\"Reverse\",ret['imputations'][0,:])\n",
    "                #print(\"ForwardOnly\",ret_f['imputations'][0,:])\n",
    "                #print(\"Original\",y.shape)\n",
    "                #print(y[0,:])\n",
    "                #print(\"Mask\",testMask.shape)\n",
    "                #print(testMask[0,:])\n",
    "                RLoss=RLoss+ret['loss']\n",
    "                FLoss=FLoss+ret_f['loss']\n",
    "    #             testMask=testMask.cuda()\n",
    "    #             y=y.cuda()\n",
    "                #print(\"Output\",ret['imputations'].shape)\n",
    "                #print(\"Output\",ret_f['imputations'].shape)\n",
    "                #print(\"Output mask\",testMask.shape)\n",
    "                outputBMI=ret['imputations'] * testMask\n",
    "                outputBMIF=ret_f['imputations'] * testMask\n",
    "                #print(\"outputBMI\",outputBMI.shape)\n",
    "                #print(outputBMI[0])\n",
    "                #print(\"outputBMIF\",outputBMIF.shape)\n",
    "                #print(outputBMIF[0])\n",
    "                mseLoss=mseLoss+ (torch.sum(torch.abs(outputBMI-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                mseLossF=mseLossF+ (torch.sum(torch.abs(outputBMIF-y)))/ (torch.sum(testMask) + 1e-5)\n",
    "                #print(\"RMSELoss Revrese: \",mseLoss)\n",
    "                #print(\"RMSELoss Forward: \",mseLossF)\n",
    "                outBmi, outBmiF,inBmi = plotBmi(outputBMI, outputBMIF, y, testMask)\n",
    "                oBmi.extend(outBmi)\n",
    "                oBmiF.extend(outBmiF)\n",
    "                iBmi.extend(inBmi)\n",
    "\n",
    "            TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "        mseLoss = mseLoss/TBatches\n",
    "        mseLossF = mseLossF/TBatches\n",
    "    #print(\"===================================\")\n",
    "    oBmi=np.asarray(oBmi)\n",
    "    iBmi=np.asarray(iBmi)\n",
    "    loss = (oBmi - iBmi)\n",
    "    loss=np.asarray([abs(number) for number in loss])\n",
    "    variance = sum([((x - mseLoss) ** 2) for x in loss]) / len(loss) \n",
    "    res = variance ** 0.5\n",
    "    ci=1.96*(res/(math.sqrt(len(loss))))\n",
    "\n",
    "    #print(\"Val R Loss:\",RLoss)\n",
    "    print(\"CI\",ci)\n",
    "    print(\"MAE Loss Reverse:\",mseLoss)\n",
    "    print(\"Total BMI values\",samples)\n",
    "    print(\"Deleted BMIs\",pids)\n",
    "    print(\"Missing%\",pids/samples)\n",
    "    #print(\"MAE Loss Forward:\",mseLossF)\n",
    "    #print(outputBMI)\n",
    "    return oBmi,oBmiF,iBmi\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBmi(outBmi,outBmiF, inBmi, testMask):\n",
    "    \n",
    "    outBmi = outBmi.cpu().detach().numpy()\n",
    "    outBmiF = outBmiF.cpu().detach().numpy()\n",
    "    inBmi = inBmi.cpu().detach().numpy()\n",
    "    testMask = testMask.cpu().detach().numpy()\n",
    "    \n",
    "    #import matplotlib.pyplot as plt\n",
    "    #%matplotlib inline\n",
    "    #from matplotlib.ticker import MultipleLocator\n",
    "    outBmi=outBmi[np.nonzero(testMask)]\n",
    "    outBmiF=outBmiF[np.nonzero(testMask)]\n",
    "    inBmi=inBmi[np.nonzero(testMask)]\n",
    "    \n",
    "    #print(outBmi)\n",
    "    #print(inBmi)\n",
    "    #print(outAge)\n",
    "    #print(outSex)\n",
    "    \n",
    "    #print(sex.shape,age.shape)\n",
    "    \n",
    "    return outBmi,outBmiF,inBmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evalFull(args, model):\n",
    "    model.eval()\n",
    "    \n",
    "    RLoss=0\n",
    "    TBatches=0\n",
    "    oBmi=[]\n",
    "    iBmi=[]\n",
    "    oAge=[]\n",
    "    oSex=[]\n",
    "   \n",
    "    with T.autograd.no_grad():\n",
    "        for i in ['M','F']:\n",
    "            files = 'cond'+i+'val.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+G+'val.csv'\n",
    "\n",
    "            maskFiles = 'mask'+i+'val.csv'\n",
    "            #print(files)\n",
    "\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            #orig = CSVDataset('C:\\\\Users/mehak/Desktop/testganAggOrig.csv', int(args.seq_len*500),1356100,args.seq_len)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            #origLoader = DataLoader(orig,batch_size=1,num_workers=0, shuffle=False)\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "\n",
    "            loss={}\n",
    "\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                pids=data[1]\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]   \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "    #             print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "                #print(\"Data:\",data[0,0,:])\n",
    "                #print(decay.shape)\n",
    "\n",
    "                ret_f, ret = run_on_batch(model,data,mask,decay,rdecay, args, optimizer=None)#,bmi_norm)\n",
    "                RLoss=RLoss+ret['loss']\n",
    "\n",
    "                #T.cuda.empty_cache()\n",
    "                #paramsE=list(model['e'].parameters())\n",
    "                #paramsG=list(model['g'].parameters())\n",
    "                #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])  \n",
    "            TBatches=TBatches+batch_idx+1\n",
    "        RLoss = RLoss/TBatches\n",
    "    #print(\"===================================\")\n",
    "    print(\"Val R Loss:\",RLoss)\n",
    "    #print(outputBMI)\n",
    "    return RLoss\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(args, model):\n",
    "    ''' Run a single epoch\n",
    "    '''\n",
    "\n",
    "    trainLoss=[]\n",
    "    valLoss=[]\n",
    "    \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(patience=args.patience, verbose=True)\n",
    "    \n",
    "    #define optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "    \n",
    "    if args.resume_training:\n",
    "        checkpoint = T.load(save_path)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        early_stopping(16.972769, model, optimizer, save_path)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #for evrey epoch\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "    \n",
    "        #Running Losses\n",
    "        RLoss=0\n",
    "        TBatches=0  \n",
    "        print(\"=============EPOCH=================\")\n",
    "       \n",
    "        for i in ['M','F']:\n",
    "       \n",
    "            files = 'cond'+i+'train.csv'\n",
    "\n",
    "    #         drugFiles = 'drug'+G+'train.csv'\n",
    "\n",
    "            maskFiles = 'mask'+i+'train.csv'\n",
    "\n",
    "            #print(files)\n",
    "            #print(maskFiles)\n",
    "            #print(\"====================New File========================\")\n",
    "            dataset = CSVDataset(files, int(args.seq_len*BATCH_SIZE),1356100,args.seq_len,flag=0)\n",
    "            maskDataset = CSVDataset(maskFiles, int(args.seq_len*BATCH_SIZE),1356100, args.seq_len,flag=1)\n",
    "\n",
    "            loader = DataLoader(dataset,batch_size=1,num_workers=0, shuffle=False)#number of times getitem is called in one iteration\n",
    "            maskLoader = DataLoader(maskDataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "            #for every batch\n",
    "            for batch_idx, allData in enumerate(zip(loader, maskLoader)):\n",
    "                #bmi_norm=dataset.bmi_norm\n",
    "                #print('batch: {}'.format(batch_idx))\n",
    "                data,mask=allData\n",
    "                pids=data[1]\n",
    "                data=data[0]\n",
    "                data=data[:,:,:,1:]\n",
    "    #             if args.female:\n",
    "    #                 data=data[:,:,:,1:229]#Female = 1:229, Male = 1:334] \n",
    "\n",
    "    #             if args.male:\n",
    "    #                 data=data[:,:,:,1:611]#Female = 1:229, Male = 1:334]\n",
    "\n",
    "                decay=mask[:,:,:,6]\n",
    "                rdecay=mask[:,:,:,7]    \n",
    "                bmi=mask[:,:,:,5]\n",
    "                mask=mask[:,:,:,4]\n",
    "\n",
    "    #             print(data.shape)\n",
    "                bmi=bmi.unsqueeze(3)\n",
    "    #             print(bmi.shape)\n",
    "    #             print(bmi[0,0,:,0])\n",
    "\n",
    "                data=torch.cat((data,bmi),dim=3)\n",
    "#                 print(data.shape)\n",
    "    #             print(data[0,0,:,228])\n",
    "\n",
    "                data=data.squeeze()\n",
    "                mask=mask.squeeze()\n",
    "                decay=decay.squeeze()\n",
    "                rdecay=rdecay.squeeze()\n",
    "                bmi=bmi.squeeze()\n",
    "#                 print(\"Data:\",data.shape)\n",
    "                #print(decay.shape)\n",
    "        \n",
    "\n",
    "\n",
    "                ret_f, ret = run_on_batch(model,data,mask,decay,rdecay, args, optimizer)#,bmi_norm)\n",
    "                RLoss=RLoss+ret['loss'].item()\n",
    "\n",
    "                #T.cuda.empty_cache()\n",
    "                #paramsE=list(model['e'].parameters())\n",
    "                #paramsG=list(model['g'].parameters())\n",
    "                #print(\"AFTER PARAM\",paramsE[0][20],paramsG[8][0][0])   \n",
    "\n",
    "            TBatches=TBatches+batch_idx+1\n",
    "        #print(TBatches)\n",
    "        #print(\"File:\", i, \"loss_R:\", \"%.4f\"%RLoss/(batch_idx+1), \"loss_G:\", \"%.4f\"%GLoss/(batch_idx+1), \"loss_D:\", \"%.4f\"%DLoss/(batch_idx+1))\n",
    "            #print(len(encoded))\n",
    "        RLoss=RLoss/TBatches\n",
    "        \n",
    "        print(\"EPOCH:\", epoch, \"loss_R:\", \"%.4f\"%RLoss)\n",
    "        \n",
    "        trainLoss.append(RLoss)\n",
    "\n",
    "        valid_loss = run_evalFull(args, model)\n",
    "        \n",
    "        valLoss.append(valid_loss)\n",
    "        #plotBmi(outBmi , inBmi)\n",
    "\n",
    "        \n",
    "        #if epoch<1 or epoch >5:\n",
    "        if not (T.isnan(valid_loss)):\n",
    "            early_stopping(valid_loss, model, optimizer, save_path)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        #plot_grad_flow(model['e'].named_parameters())\n",
    "        #plot_grad_flow(model['g'].named_parameters())\n",
    "        #plot_grad_flow(model['d'].named_parameters())\n",
    "    return trainLoss, valLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "    \n",
    "    train_on_gpu = T.cuda.is_available()\n",
    "    if train_on_gpu:\n",
    "        print('Training on GPU.')\n",
    "    else:\n",
    "        print('No GPU available, training on CPU.')\n",
    "        \n",
    "    model = BRITS2(args)\n",
    "    \n",
    "    #print(\"Model\",model)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    if args.resume_training:\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        trainLoss, valLoss = run_epoch(args, model) \n",
    "        #trainLoss, valLoss = run_evalFull(args, model) \n",
    "        return trainLoss, valLoss\n",
    "    \n",
    "    elif args.train:\n",
    "        trainLoss, valLoss = run_epoch(args, model) \n",
    "        return trainLoss, valLoss\n",
    "        \n",
    "    elif args.evalImp:\n",
    "        #load Model\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        oBmi, oBmiF, iBmi = imputation_test(args, model,args.missingRate)\n",
    "        \n",
    "    elif args.evalPred:\n",
    "        #load Model\n",
    "        checkpoint = T.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
    "        optimizer.load_state_dict(checkpoint['trainer'])\n",
    "        oBmi, oBmiF, iBmi = pred_test(args, model,args.pred_len)\n",
    "        \n",
    "        #return oBmi, oBmiF, iBmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n",
      "MSE Loss Reverse: tensor(2.8642, device='cuda:0')\n",
      "Missing%:  0.21005759069049457\n"
     ]
    }
   ],
   "source": [
    "#trainLoss, valLoss = run(ARGS)\n",
    "#oBmi, oBmiF, iBmi, oAge, oSex = run(ARGS)\n",
    "run(ARGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Atena] *",
   "language": "python",
   "name": "conda-env-Atena-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

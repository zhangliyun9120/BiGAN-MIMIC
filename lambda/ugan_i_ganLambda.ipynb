{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "SEQ_LEN = 40\n",
    "D_HID_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combination(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Combination, self).__init__()\n",
    "        self.build(input_size)\n",
    "\n",
    "    def build(self, input_size):\n",
    "        self.W1 = Parameter(torch.Tensor(5, input_size))\n",
    "        self.b1 = Parameter(torch.Tensor(5))\n",
    "        \n",
    "        self.W2 = Parameter(torch.Tensor(1, 5))\n",
    "        self.b2 = Parameter(torch.Tensor(1))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv1 = 1. / math.sqrt(self.W1.size(0))\n",
    "        self.W1.data.uniform_(-stdv1, stdv1)\n",
    "        \n",
    "        stdv2 = 1. / math.sqrt(self.W2.size(0))\n",
    "        self.W2.data.uniform_(-stdv2, stdv2)\n",
    "        \n",
    "        if self.b1 is not None:\n",
    "            self.b1.data.uniform_(-stdv1, stdv1)\n",
    "            \n",
    "        if self.b2 is not None:\n",
    "            self.b2.data.uniform_(-stdv2, stdv2)\n",
    "\n",
    "    def forward(self, d):\n",
    "        #print(d.size())\n",
    "        gamma = F.relu(F.linear(d, self.W1, self.b1))\n",
    "        #print(gamma.size())\n",
    "        gamma = F.relu(F.linear(gamma, self.W2, self.b2))\n",
    "        #print(gamma.size())\n",
    "        gamma = torch.exp(-gamma)\n",
    "        return gamma\n",
    "\n",
    "class TemporalDecay(nn.Module):\n",
    "    def __init__(self, input_size,RNN_HID_SIZE):\n",
    "        super(TemporalDecay, self).__init__()\n",
    "        self.build(input_size,RNN_HID_SIZE)\n",
    "\n",
    "    def build(self, input_size,RNN_HID_SIZE):\n",
    "        self.W = Parameter(torch.Tensor(RNN_HID_SIZE, input_size))\n",
    "        self.b = Parameter(torch.Tensor(RNN_HID_SIZE))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.W.size(0))\n",
    "        self.W.data.uniform_(-stdv, stdv)\n",
    "        if self.b is not None:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, d):\n",
    "        gamma = F.relu(F.linear(d, self.W, self.b))\n",
    "        gamma = torch.exp(-gamma)\n",
    "        return gamma\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.rnn_cell = nn.LSTMCell(1, D_HID_SIZE)\n",
    "        self.regression1 = nn.Linear(D_HID_SIZE, 5)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.regression2 = nn.Linear(5, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, values, masks, args, direct):\n",
    "        \n",
    "        h = Variable(torch.zeros((values.size()[0], D_HID_SIZE)))\n",
    "        c = Variable(torch.zeros((values.size()[0], D_HID_SIZE)))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "            values, masks = values.cuda(), masks.cuda()\n",
    "            \n",
    "        scoresSig=[]\n",
    "        scoresReg=[]\n",
    "        missing=[]\n",
    "        if(direct==\"forward\"):\n",
    "\n",
    "            for t in range(SEQ_LEN):\n",
    "                #print(\"===============\",t,\"======================\")\n",
    "                x = values[:, t]\n",
    "                x=x.unsqueeze(dim=1)\n",
    "                m = masks[:, t]\n",
    "                #print(\"Input\",x.size())\n",
    "                #print(\"Input\",x[0])\n",
    "\n",
    "                x_h = self.regression1(h)\n",
    "                x_h = self.leaky(x_h)\n",
    "                x_h = self.regression2(x_h)\n",
    "                x_h2 = self.sig(x_h)\n",
    "                #print(\"Discriminator output\",x_h.shape)\n",
    "\n",
    "                #print(\"Output regression\",x_h.size())\n",
    "                #print(\"Mask\",m.size())\n",
    "\n",
    "                m=m.unsqueeze(dim=1)\n",
    "                \n",
    "                #print(\"i am here\")\n",
    "\n",
    "                h, c = self.rnn_cell(x, (h, c))\n",
    "                #print(\"i am here\")\n",
    "\n",
    "                #imputations.append(x_c[:,316].unsqueeze(dim = 1))\n",
    "                scoresSig.append(x_h2[:,0].unsqueeze(dim = 1))\n",
    "                scoresReg.append(x_h[:,0].unsqueeze(dim = 1))\n",
    "                #print(\"i am here\")\n",
    "                missing.append(m)\n",
    "                #print(\"i am here\")\n",
    "                #print(\"to be appended\",m.size())\n",
    "                #print(\"Imputations\",len(imputations))\n",
    "                #print(\"Scores\",scores[0].size())\n",
    "        \n",
    "        elif(direct==\"backward\"):\n",
    "\n",
    "            for t in range(SEQ_LEN-1,-1,-1):\n",
    "                #print(\"===============\",t,\"======================\")\n",
    "                x = values[:, t]\n",
    "                x=x.unsqueeze(dim=1)\n",
    "                m = masks[:, t]\n",
    "                #print(\"Input\",x.size())\n",
    "                #print(\"Input\",x[0])\n",
    "\n",
    "                x_h = self.regression1(h)\n",
    "                x_h = self.leaky(x_h)\n",
    "                x_h = self.regression2(x_h)\n",
    "                x_h2 = self.sig(x_h)\n",
    "                #print(\"Discriminator output\",x_h.shape)\n",
    "\n",
    "                #print(\"Output regression\",x_h.size())\n",
    "                #print(\"Mask\",m.size())\n",
    "\n",
    "                m=m.unsqueeze(dim=1)\n",
    "                \n",
    "                #print(\"d\",d[:,0].unsqueeze(dim=1).size())\n",
    "\n",
    "                h, c = self.rnn_cell(x, (h, c))\n",
    "\n",
    "                #imputations.append(x_c[:,316].unsqueeze(dim = 1))\n",
    "                scoresSig.append(x_h2[:,0].unsqueeze(dim = 1))\n",
    "                scoresReg.append(x_h[:,0].unsqueeze(dim = 1))\n",
    "                missing.append(m)\n",
    "                #print(\"to be appended\",m.size())\n",
    "                #print(\"Imputations\",len(imputations))\n",
    "                #print(\"Scores\",scores[0].size())\n",
    "        \n",
    "        scoresSig = torch.cat(scoresSig, dim = 1)\n",
    "        scoresReg = torch.cat(scoresReg, dim = 1)\n",
    "        missing = torch.cat(missing, dim = 1)\n",
    "        #print(\"Scores\",len(scores),scores[0].size())\n",
    "        return {'scoresSig': scoresSig, 'scoresReg': scoresReg, 'missing':missing}\n",
    "        \n",
    "class UGAN(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super(UGAN, self).__init__()\n",
    "        if args.air:\n",
    "            self.RNN_HID_SIZE = 10\n",
    "            self.NFEATURES = 14\n",
    "            self.var=2\n",
    "        if args.mimic:\n",
    "            self.RNN_HID_SIZE = 10\n",
    "            self.NFEATURES = 20\n",
    "            self.var=2\n",
    "        if args.ehr:\n",
    "            self.RNN_HID_SIZE = 400\n",
    "            self.NFEATURES = 812\n",
    "            self.var=811\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        self.rnn_cell = nn.LSTMCell(self.NFEATURES + 1, self.RNN_HID_SIZE)\n",
    "\n",
    "        #self.regression = nn.Linear(RNN_HID_SIZE, 35)\n",
    "        self.regression = nn.Linear(self.RNN_HID_SIZE, 1)\n",
    "        self.temp_decay = TemporalDecay(input_size = self.NFEATURES,RNN_HID_SIZE = self.RNN_HID_SIZE)\n",
    "        #self.comb_factor = Combination(input_size = 1)\n",
    "        \n",
    "\n",
    "        #self.out = nn.Linear(RNN_HID_SIZE, 1)\n",
    "\n",
    "    def forward(self, values, masks, deltas, args, direct):\n",
    "        # Original sequence with 24 time steps\n",
    "        deltas=deltas.unsqueeze(dim=2).repeat(1,1,self.NFEATURES)\n",
    "        #print(\"deltas\",deltas[0])\n",
    "\n",
    "        #evals = data[direct]['evals']\n",
    "        #eval_masks = data[direct]['eval_masks']\n",
    "\n",
    "        #labels = data['labels'].view(-1, 1)\n",
    "        #is_train = data['is_train'].view(-1, 1)\n",
    "\n",
    "        h = Variable(torch.zeros((values.size()[0], self.RNN_HID_SIZE)))\n",
    "        c = Variable(torch.zeros((values.size()[0], self.RNN_HID_SIZE)))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h, c = h.cuda(), c.cuda()\n",
    "            values, masks, deltas = values.cuda(), masks.cuda(), deltas.cuda()\n",
    "\n",
    "        x_loss = 0.0\n",
    "        #y_loss = 0.0\n",
    "\n",
    "        imputations = []\n",
    "        combFactor=[]\n",
    "        missing=[]\n",
    "        originals=[]\n",
    "        if(direct==\"forward\"):\n",
    "\n",
    "            for t in range(SEQ_LEN):\n",
    "                #print(\"===============\",t,\"======================\")\n",
    "                x = values[:, t, :]\n",
    "                m = masks[:, t]\n",
    "                d = deltas[:, t]\n",
    "                #print(\"d\",d[:,0].unsqueeze(dim=1).size())\n",
    "                #print(\"d\",d[7,:])\n",
    "                #print(d[:,0])\n",
    "\n",
    "                gamma = self.temp_decay(d)\n",
    "                #print(\"Gamma\",gamma.size())\n",
    "                h = h * gamma\n",
    "                #print(\"h\",h.size())\n",
    "                x_h = self.regression(h)\n",
    "                #print(\"Regression output\",x_h[0,:])\n",
    "\n",
    "                #print(\"Output regression\",x_h.size())\n",
    "                #print(\"Mask\",m.size())\n",
    "\n",
    "                #x_c =  m * x +  (1 - m) * x_h\n",
    "                x[:,self.var] =  x[:,self.var]*m + (1-m)*x_h[:,0]\n",
    "                x_c=x\n",
    "                #print(\"Complement Vector\",x_c.size())\n",
    "                #print(\"Complement Vector\",x_c[0,316])\n",
    "                \n",
    "                #comb=self.comb_factor(d[:,0].unsqueeze(dim=1))\n",
    "                #print(\"Comb Output\",comb.size())\n",
    "\n",
    "                x_loss += torch.sum(torch.abs(x[:,self.var] - x_h[:,0]) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "                #print(\"X_loss\",x_loss)\n",
    "                m=m.unsqueeze(dim=1)\n",
    "\n",
    "                inputs = torch.cat([x_c, m], dim = 1)\n",
    "\n",
    "                #print(\"Next input\",inputs.size())\n",
    "\n",
    "                h, c = self.rnn_cell(inputs, (h, c))\n",
    "\n",
    "                #imputations.append(x_c[:,316].unsqueeze(dim = 1))\n",
    "                imputations.append(x_h[:,0].unsqueeze(dim = 1))\n",
    "                originals.append(x_c[:,self.var].unsqueeze(dim = 1))\n",
    "                #combFactor.append(comb)\n",
    "                missing.append(m)\n",
    "                #print(\"to be appended\",m.size())\n",
    "                #print(\"Imputations\",len(imputations))\n",
    "                #print(\"Imputations\",combFactor[0].size())\n",
    "                \n",
    "        elif(direct==\"backward\"):\n",
    "            #print(\"BACKWARD\")\n",
    "            for t in range(SEQ_LEN-1,-1,-1):\n",
    "                #print(\"===============\",t,\"======================\")\n",
    "                x = values[:, t, :]\n",
    "                m = masks[:, t]\n",
    "                d = deltas[:, t]\n",
    "                #print(\"Input\",x.size())\n",
    "\n",
    "                gamma = self.temp_decay(d)\n",
    "                #print(\"Gamma\",gamma[0])\n",
    "                h = h * gamma\n",
    "                #print(\"h\",h.size())\n",
    "                x_h = self.regression(h)\n",
    "\n",
    "                #print(\"Output regression\",x_h.size())\n",
    "                #print(\"Mask\",m.size())\n",
    "                \n",
    "                #print(\"Regression output\",x_h[0,:])\n",
    "\n",
    "                #x_c =  m * x +  (1 - m) * x_h\n",
    "                x[:,self.var] =  x[:,self.var]*m + (1-m)*x_h[:,0]\n",
    "                x_c=x\n",
    "                #print(\"Complement Vector\",x_c.size())\n",
    "                #print(\"Complement Vector\",x_c[0,316])\n",
    "                \n",
    "                #comb=self.comb_factor(d[:,0].unsqueeze(dim=1))\n",
    "\n",
    "                x_loss += torch.sum(torch.abs(x[:,self.var] - x_h[:,0]) * m) / (torch.sum(m) + 1e-5)\n",
    "\n",
    "                #print(\"X_loss\",x_loss)\n",
    "                m=m.unsqueeze(dim=1)\n",
    "\n",
    "                inputs = torch.cat([x_c, m], dim = 1)\n",
    "\n",
    "                #print(\"Next input\",inputs.size())\n",
    "\n",
    "                h, c = self.rnn_cell(inputs, (h, c))\n",
    "\n",
    "                #imputations.append(x_c[:,316].unsqueeze(dim = 1))\n",
    "                imputations.append(x_h[:,0].unsqueeze(dim = 1))\n",
    "                originals.append(x_c[:,self.var].unsqueeze(dim = 1))\n",
    "                #combFactor.append(comb)\n",
    "                missing.append(m)\n",
    "                #print(\"Imputations\",imputations[0].size())\n",
    "\n",
    "        imputations = torch.cat(imputations, dim = 1)\n",
    "        originals = torch.cat(originals, dim = 1)\n",
    "        #combFactor = torch.cat(combFactor, dim = 1)\n",
    "        missing = torch.cat(missing, dim = 1)\n",
    "        #print(\"Final Imputations\",imputations.size())\n",
    "        #print(\"Final Combs\",combFactor.size())\n",
    "        #print(\"Final Missing\",missing.size())\n",
    "\n",
    "        return {'loss': x_loss / SEQ_LEN , 'originals':originals, 'imputations': imputations, 'missing':missing}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Atena] *",
   "language": "python",
   "name": "conda-env-Atena-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
